{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2C_6nI_fpdA"
      },
      "source": [
        "# Homework-4\n",
        "Recurrent Neural Networks  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmiFELrxfumq"
      },
      "source": [
        "## Problem 1:\n",
        "\n",
        "Answer each of the following questions.\n",
        "\n",
        "(A) Both CNNs and RNNs share parameters—but in different ways. Explain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "INSERT ANSWER HERE\n",
        "\n",
        "CNN parameters are used for dimensional problems such as images and videos, which kernel applies to input(image,video), allowing weights to find patterns without considering position.\n",
        "RNN are more used in sequential data(weather forecast,stock),same weights are used at every time step in the sequence.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "(B) What is the basic idea underlying how we learn meaningful word embeddings? How does this relate to the idea of self-supervised learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "INSERT ANSWER HERE\n",
        "\n",
        "basic structure of word embedding is predicting word from its context, finding similarity. to do this word embedding place words as vectorized space, and find similarity from the word context. and word embedding does not labeled for prediction(ex, 1 or 0 ) since its struture is not finding yes or no for similarity. and self supervise learning's structure is nto derive various supervision signals based on the data, which alignes with what word embedding does. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(C) Consider an LSTM layer that accepts a batch of arbitrary size, with 40 time steps and 3 features. The LSTM layer has input shape `[None, 40, 3]`. Suppose its output is shape `[None, 2]`. Find the number of parameters in the LSTM layer, explaining your thinking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "INSERT ANSWER HERE\n",
        "\n",
        "input shape has 40 sequence(time steps) and each has three features. output shape has size 2, which LSTM has two units, also four gates for LSTM.\n",
        "weight for input( 3 features) : 3*2 = 6 ; 3 weights per unit and there are two units so 3*2 = 6 units per gate.  hidden state weight : 2*2 ; two weights per unit and there are two units wich leads to four weights per gate. two bias per gate.\n",
        "so, 4(6+4+2) = 48\n",
        "\n",
        "total params : 48\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem-2: \n",
        "\n",
        " Begin by reviewing Andrej Karpathy's famous blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
        "\n",
        "Provide a three-paragraph summary of the article, describing Karpathy's main points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "INSERT ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem-3: \n",
        "\n",
        "Write code for a character-based RNN (i.e. LSTM or GRU, not simple_RNN) in PyTorch. You can use his source code and any code online to assist. If you do, just reference where you sought help from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Choose a large text corpus, such as a collection of novels from [Project Gutenburg](https://www.gutenberg.org/). You can use other data, but you should explain where your data comes from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "INSERT ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Perform any necessary preprocessing, explaining what steps you take. In particular, what forms of normalization do you use? Do you define characters with special meaning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "for preprocessing, it has processed characters to be lowercase(not case sensitive), removing witespace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INSERT CODE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "\n",
        "with open(\"mobi.rtf\", encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "text = raw_text.lower()\n",
        "all_chars = sorted(list(set(c for c in text if c in string.printable)))\n",
        "vocab_size = len(all_chars)\n",
        "\n",
        "char_idx = {ch: idx for idx, ch in enumerate(all_chars)}\n",
        "idx_char = {idx: ch for ch, idx in char_idx.items()}\n",
        "\n",
        "def encode(text):\n",
        "    return [char_idx[c] for c in text if c in char_idx]\n",
        "\n",
        "def decode(indices):\n",
        "    return ''.join(idx_char[i] for i in indices)\n",
        "\n",
        "encoded_text = encode(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Efficiently load and batch the dataset for training using a `DataLoader`. Make sure to reserve some of the data for validation and testing. Describe how you handle batching and sequence lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INSERT CODE\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "encoded_text = encode(text)\n",
        "encoded_text = torch.tensor(encoded_text, dtype=torch.long)\n",
        "\n",
        "n = len(encoded_text)\n",
        "train_len = int(0.8 * n)\n",
        "val_len = int(0.05 * n)\n",
        "test_len = n - train_len - val_len\n",
        "\n",
        "train_seq = encoded_text[:train_len]\n",
        "val_seq = encoded_text[train_len:train_len+val_len]\n",
        "test_seq = encoded_text[train_len+val_len:]\n",
        "\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, seq_len):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx+self.seq_len]\n",
        "        y = self.data[idx+1:idx+self.seq_len+1]\n",
        "        return x, y\n",
        "\n",
        "SEQ_LEN = 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = CharDataset(train_seq, SEQ_LEN)\n",
        "val_dataset = CharDataset(val_seq, SEQ_LEN)\n",
        "test_dataset = CharDataset(test_seq, SEQ_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- Define your RNN model. Discuss the number of layers, hidden units, and the type of RNN cells you use. What is the total number of parameters in your model? Explain the rationale behind your architectural choices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INSERT CODE\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN1(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_size=512, num_layers=2, rnn_type='lstm'):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        if rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(input_size=embedding_dim,\n",
        "                               hidden_size=hidden_size,\n",
        "                               num_layers=num_layers,\n",
        "                               batch_first=True)\n",
        "        elif rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(input_size=embedding_dim,\n",
        "                              hidden_size=hidden_size,\n",
        "                              num_layers=num_layers,\n",
        "                              batch_first=True)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)                  \n",
        "        output, hidden = self.rnn(x, hidden)    \n",
        "        output = self.fc(output)              \n",
        "        return output, hidden\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- Write the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 179/179 [00:58<00:00,  3.07it/s, loss=0.119]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 1.1010 | Val Loss: 3.3128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 179/179 [00:55<00:00,  3.22it/s, loss=0.0995]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 0.1039 | Val Loss: 3.6960\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 179/179 [00:54<00:00,  3.25it/s, loss=0.0929]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 0.0913 | Val Loss: 3.8918\n"
          ]
        }
      ],
      "source": [
        "# INSERT CODE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCHS = 3\n",
        "CLIP_GRAD = 5.0  # to prevent exploding gradients\n",
        "\n",
        "# --- Initialize model, loss, optimizer ---\n",
        "vocab_size = len(char2idx)\n",
        "model = RNN1(vocab_size=vocab_size, embedding_dim=256, hidden_size=512, num_layers=2, rnn_type='lstm').to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "# --- Training ---\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
        "\n",
        "    for inputs, targets in pbar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs, _ = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "        # Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if CLIP_GRAD:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- Monitor and report on the training progress by tracking the loss. After training, evaluate the model's performance using a suitable evaluation metric (e.g., perplexity) on a validation dataset or a held-out portion of the training data. Discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 179/179 [00:57<00:00,  3.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 0.0867 | Val Loss: 4.3067 | Perplexity: 74.19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 179/179 [00:55<00:00,  3.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 0.0836 | Val Loss: 4.1532 | Perplexity: 63.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 179/179 [00:57<00:00,  3.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 0.0817 | Val Loss: 4.2255 | Perplexity: 68.41\n"
          ]
        }
      ],
      "source": [
        "# INSERT CODE\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_perplexities = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs, _ = model(inputs)\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # --- Validation ---\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    perplexity = math.exp(avg_val_loss)\n",
        "    val_perplexities.append(perplexity)\n",
        "\n",
        "    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Perplexity: {perplexity:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- Specify the hyper-parameters (for example, model hyper-parameters, as well as sampling size and beam width from below) used in your model. Find suitable settings using a validation split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INSERT CODE\n",
        "hyperparams = {\n",
        "    'embedding_dim': 256,\n",
        "    'hidden_size': 512,\n",
        "    'num_layers': 2,\n",
        "    'rnn_type': 'lstm',\n",
        "    'seq_len': 100,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 0.003,\n",
        "    'epochs': 20,\n",
        "    'temperature': 0.8,\n",
        "    'sampling_length': 500,\n",
        "    'clip_grad_norm': 5.0\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- Implement a text generation function using the trained RNN model. Provide a prompt or seed text, and use the RNN to generate a sequence of characters. Experiment with different prompt texts and observe how the generated text changes. Discuss any interesting patterns or observations you make during the text generation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Call me Ishmael. some years ago\\'97never mind how long precisely\\'97having\\\n",
            "little or no money in my purse, and nothing particular to interest me\\\n",
            "on shore, i thought i would sail about a little and see the watery part\\\n",
            "of the world. it is a way i have of driving off the spleen and\\\n",
            "regulating the circulation. whenever i find myself growing grim about\\\n",
            "the mouth; whenever it is a damp, drizzly november in my soul; whenever\\\n",
            "i find myself involuntarily pausing before coffin warehouses, and\\\n",
            "bringing up the rear o\n",
            "\n",
            "The ocean with me.\\\n",
            "\\\n",
            "there now is your insular city of the manhattoes, belted round by\\\n",
            "wharves as indian isles by coral reefs\\'97commerce surrounds it with her\\\n",
            "surf. right and left, the streets take you waterward. its extreme\\\n",
            "downtown is the battery, where that noble mole is washed by waves, and\\\n",
            "cooled by breezes, which a few hours previous were out of sight of\\\n",
            "land. look at the crowds of water-gazers there.\\\n",
            "\\\n",
            "circumambulate the city of a dreamy sabbath afternoon. go from corlears\\\n",
            "hook to coentie\n",
            "\n",
            "Whale\\\n",
            "rived to desks. how then is this? are\\\n",
            "the green fields gone? what do they here?\\\n",
            "\\\n",
            "but look! here come more crowds, pacing straight for the water, and\\\n",
            "seemingly bound for a dive. strange! nothing will content them but the\\\n",
            "commodore on the quarter-deck gets his atmosphere at second hand from\\\n",
            "the sailors on the forecastle. he thinks he breathes it first; but not\\\n",
            "so. in much the same way do the commonalty lead their leaders in many\\\n",
            "other things, at the same time that the leaders little susp\n"
          ]
        }
      ],
      "source": [
        "# INSERT CODE\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_text(model, start_text='start', length=500, temperature=0.8, device='cpu'):\n",
        "    model.eval()\n",
        "    input_indices = [char_idx[c] for c in start_text.lower() if c in char_idx]\n",
        "    input_tensor = torch.tensor([input_indices], dtype=torch.long).to(device)\n",
        "\n",
        "    generated = start_text\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            logits = output[:, -1, :] / temperature  # shape: (1, vocab_size)\n",
        "            probs = F.softmax(logits, dim=-1).squeeze()\n",
        "            next_char_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            next_char = idx_char[next_char_idx]\n",
        "            generated += next_char\n",
        "            input_tensor = torch.tensor([[next_char_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "    return generated\n",
        "\n",
        "print(generate_text(model, start_text=\"Call me Ishmael. \", temperature=0.8, length=500))\n",
        "print()\n",
        "print(generate_text(model, start_text=\"The ocean\", temperature=0.7, length=500))\n",
        "print()\n",
        "print(generate_text(model, start_text=\"Whale\", temperature=1.0, length=500))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem-4: Optional extra credit \n",
        " \n",
        "Up to +3 bonus points \n",
        "\n",
        "* Repeat the text generation process from the previous problem, but do it with a transformer architecture rather than an LSTM/GRU (you can use word tokens instead of character tokens if you prefer). Use the provided mini-GPT lab for reference.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem-5: Optional extra credit \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- (+1 bonus points): Now with your model trained, implement top-k and nucleus sampling. Again, provide a prompt or seed text, and use the RNN to generate a sequence of characters. Experiment with different prompt texts and observe how the generated text changes. Discuss any interesting patterns or observations you make during the text generation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INSERT CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- (+1 bonus points): Now with your model trained, implement beam search. Again, provide a prompt or seed text, and use the RNN to generate a sequence of characters. Experiment with different prompt texts and observe how the generated text changes. Discuss any interesting patterns or observations you make during the text generation process.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INSERT CODE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
